{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms as tf\n",
    "from os import walk\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from matplotlib import image\n",
    "from torchvision import transforms\n",
    "# Set ipython's max row display\n",
    "pd.set_option('display.max_row', 1000)\n",
    "\n",
    "# Set iPython's max column width to 50\n",
    "pd.set_option('display.max_columns', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('train_df2.csv')\n",
    "test_df = pd.read_csv('test_df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Finding Label</th>\n",
       "      <th>Image Index</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>45174</th>\n",
       "      <td>1</td>\n",
       "      <td>00013925_002.png</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39526</th>\n",
       "      <td>1</td>\n",
       "      <td>00012176_013.png</td>\n",
       "      <td>2991.0</td>\n",
       "      <td>2992.0</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76024</th>\n",
       "      <td>1</td>\n",
       "      <td>00025303_036.png</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>2021.0</td>\n",
       "      <td>0.194311</td>\n",
       "      <td>0.194311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20339</th>\n",
       "      <td>8</td>\n",
       "      <td>00006391_001.png</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25440</th>\n",
       "      <td>5</td>\n",
       "      <td>00007916_004.png</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20854</th>\n",
       "      <td>1</td>\n",
       "      <td>00006547_003.png</td>\n",
       "      <td>2991.0</td>\n",
       "      <td>2992.0</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46441</th>\n",
       "      <td>5</td>\n",
       "      <td>00014320_022.png</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2905</th>\n",
       "      <td>1</td>\n",
       "      <td>00000980_001.png</td>\n",
       "      <td>2048.0</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0.171000</td>\n",
       "      <td>0.171000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53949</th>\n",
       "      <td>5</td>\n",
       "      <td>00016778_007.png</td>\n",
       "      <td>2544.0</td>\n",
       "      <td>3056.0</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3609</th>\n",
       "      <td>1</td>\n",
       "      <td>00001221_003.png</td>\n",
       "      <td>2991.0</td>\n",
       "      <td>2846.0</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Finding Label       Image Index       h       w         x         y\n",
       "45174              1  00013925_002.png  2048.0  2500.0  0.168000  0.168000\n",
       "39526              1  00012176_013.png  2991.0  2992.0  0.143000  0.143000\n",
       "76024              1  00025303_036.png  2021.0  2021.0  0.194311  0.194311\n",
       "20339              8  00006391_001.png  2048.0  2500.0  0.171000  0.171000\n",
       "25440              5  00007916_004.png  2048.0  2500.0  0.168000  0.168000\n",
       "20854              1  00006547_003.png  2991.0  2992.0  0.143000  0.143000\n",
       "46441              5  00014320_022.png  2048.0  2500.0  0.168000  0.168000\n",
       "2905               1  00000980_001.png  2048.0  2500.0  0.171000  0.171000\n",
       "53949              5  00016778_007.png  2544.0  3056.0  0.139000  0.139000\n",
       "3609               1  00001221_003.png  2991.0  2846.0  0.143000  0.143000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     49232\n",
       "4      8106\n",
       "8      7795\n",
       "5      5045\n",
       "6      2284\n",
       "3      2067\n",
       "12     1933\n",
       "0      1385\n",
       "9      1155\n",
       "13     1074\n",
       "7       903\n",
       "11      879\n",
       "10      826\n",
       "14      230\n",
       "2        81\n",
       "Name: Finding Label, dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Finding Label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (train_df['Image Index'] == '00030181_001.png').value_counts()\n",
    "# 00028247_001.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import platform\n",
    "my_path = \"../Datasets/Lungs_Dataset/Xray\" if platform.system() == 'Windows' else \"datasets/data/images\"\n",
    "filename_list = []\n",
    "for root, dirs, files in os.walk(my_path, topdown=True):\n",
    "    for name in files:\n",
    "        filename_list.append(name)\n",
    "#         with Image.open(os.path.join(\"datasets/data/images\", name)) as f:\n",
    "#             print(len(f.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True     82995\n",
       "False     3529\n",
       "Name: Image Index, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Image Index'].isin(filename_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df[train_df['Image Index'].isin(filename_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    82995\n",
       "Name: Image Index, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Image Index'].isin(filename_list).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    82995\n",
       "Name: Image Index, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Image Index'].isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df[test_df['Image Index'].isin(filename_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Finding Label</th>\n",
       "      <th>Image Index</th>\n",
       "      <th>h</th>\n",
       "      <th>w</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8572</th>\n",
       "      <td>5</td>\n",
       "      <td>00012263_007.png</td>\n",
       "      <td>2521.000000</td>\n",
       "      <td>2794.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14922</th>\n",
       "      <td>1</td>\n",
       "      <td>00017714_014.png</td>\n",
       "      <td>2544.000000</td>\n",
       "      <td>3056.000000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5920</th>\n",
       "      <td>4</td>\n",
       "      <td>00009608_042.png</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26419</th>\n",
       "      <td>14</td>\n",
       "      <td>00022021_002.png</td>\n",
       "      <td>93.189418</td>\n",
       "      <td>154.954497</td>\n",
       "      <td>267.648677</td>\n",
       "      <td>493.037037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9914</th>\n",
       "      <td>12</td>\n",
       "      <td>00013615_029.png</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20329</th>\n",
       "      <td>1</td>\n",
       "      <td>00025664_040.png</td>\n",
       "      <td>2544.000000</td>\n",
       "      <td>3056.000000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2352</th>\n",
       "      <td>13</td>\n",
       "      <td>00003528_017.png</td>\n",
       "      <td>2048.000000</td>\n",
       "      <td>2500.000000</td>\n",
       "      <td>0.168000</td>\n",
       "      <td>0.168000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19395</th>\n",
       "      <td>1</td>\n",
       "      <td>00022877_020.png</td>\n",
       "      <td>2544.000000</td>\n",
       "      <td>3056.000000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17954</th>\n",
       "      <td>10</td>\n",
       "      <td>00021047_009.png</td>\n",
       "      <td>2544.000000</td>\n",
       "      <td>3056.000000</td>\n",
       "      <td>0.139000</td>\n",
       "      <td>0.139000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17766</th>\n",
       "      <td>8</td>\n",
       "      <td>00020751_005.png</td>\n",
       "      <td>2991.000000</td>\n",
       "      <td>2992.000000</td>\n",
       "      <td>0.143000</td>\n",
       "      <td>0.143000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Finding Label       Image Index            h            w           x  \\\n",
       "8572               5  00012263_007.png  2521.000000  2794.000000    0.143000   \n",
       "14922              1  00017714_014.png  2544.000000  3056.000000    0.139000   \n",
       "5920               4  00009608_042.png  2048.000000  2500.000000    0.168000   \n",
       "26419             14  00022021_002.png    93.189418   154.954497  267.648677   \n",
       "9914              12  00013615_029.png  2048.000000  2500.000000    0.168000   \n",
       "20329              1  00025664_040.png  2544.000000  3056.000000    0.139000   \n",
       "2352              13  00003528_017.png  2048.000000  2500.000000    0.168000   \n",
       "19395              1  00022877_020.png  2544.000000  3056.000000    0.139000   \n",
       "17954             10  00021047_009.png  2544.000000  3056.000000    0.139000   \n",
       "17766              8  00020751_005.png  2991.000000  2992.000000    0.143000   \n",
       "\n",
       "                y  \n",
       "8572     0.143000  \n",
       "14922    0.139000  \n",
       "5920     0.168000  \n",
       "26419  493.037037  \n",
       "9914     0.168000  \n",
       "20329    0.139000  \n",
       "2352     0.168000  \n",
       "19395    0.139000  \n",
       "17954    0.139000  \n",
       "17766    0.143000  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RescaleImage(object):\n",
    "    \"\"\"Rescale the image in a sample to a given size.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "            \n",
    "        The samples coming into this class will have its images reduced assuming\n",
    "        the input is a h, w, c numpy array\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        h, w = image.size\n",
    "\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "        img = image.resize((new_h, new_w))\n",
    "\n",
    "        return {'image': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, label = sample['image'], sample['label']\n",
    "        image = np.array(image)\n",
    "        if len(image.shape) > 2:\n",
    "            image = image[:,:,0]\n",
    "\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C X H X W\n",
    "        return {'image': torch.FloatTensor(image).unsqueeze(0),\n",
    "#         return {'image': torch.from_numpy(image),                \n",
    "                'label': torch.from_numpy(np.array(label))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  print([Image.open('../Datasets/Lungs_Dataset/Xray/'+train_df['Image Index'][each_import]).size for each_import in range(100)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note this will return an Image object, of h and w, and its corresponding label\n",
    "class CovidLungsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, dataframe, root_dir, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img_name = os.path.join(self.root_dir,\n",
    "                                self.dataframe['Image Index'][idx])\n",
    "        my_image = Image.open(img_name)        \n",
    "        if len(my_image.size) > 2:\n",
    "            assert len(my_image.size) > 2\n",
    "        row = self.dataframe.iloc[idx]\n",
    "        label = row['Finding Label']\n",
    "        sample = {'image': my_image, 'label': label}\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_set = CovidLungsDataset(train_df, my_path, transform=transforms.Compose([\n",
    "        RescaleImage(200),\n",
    "        ToTensor()\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Tensor, torch.Size([1, 200, 200]), tensor(0))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The original data coming out of the Dataset is a dictionary, \n",
    "the image is an Image object with the corresponding label.\n",
    "The size of all of these images are 1024 by 1024.\n",
    "The RescaleImage will convert the Image object to an Image object with 200x200 in size, \n",
    "leaving the label alone.\n",
    "The ToTensor will convert the Image object with 200x200 to tensors of 1x200x200'''\n",
    "from random import randint\n",
    "a_sample = my_train_set.__getitem__(randint(0,256))\n",
    "my_image = a_sample['image']\n",
    "my_label = a_sample['label']\n",
    "#imshow will work if it's h,w,c or h,w\n",
    "#torch is c,h,w\n",
    "#line below can be used if you don't use ToTensor\n",
    "# plt.imshow(my_image)\n",
    "type(my_image), my_image.shape, my_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(my_image.squeeze(0));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import platform\n",
    "batch_loader_params = {\n",
    "    \"batch_size\": 10,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0 if platform.system() == 'Windows' else 2\n",
    "}\n",
    "dataloader = DataLoader(my_train_set, **batch_loader_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # iter(dataloader).next()\n",
    "# for i, each in enumerate(dataloader):\n",
    "#     print(each['image'].shape, each['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_samples = iter(dataloader)\n",
    "# samples = batch_samples.next()\n",
    "# datset_batch = torchvision.utils.make_grid(samples['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(20,10))\n",
    "# for index, each in enumerate(datset_batch):\n",
    "#     plt.imshow(each.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spatial_size(input_size: int, kernel_size: int, stride: int = 1, padding: int = 0):\n",
    "    # https://cs231n.github.io/convolutional-networks/\n",
    "    spatial_size = (input_size - kernel_size + 2 * padding)/stride + 1\n",
    "    assert spatial_size % 1 == 0\n",
    "    assert spatial_size > 0\n",
    "    return int(spatial_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26\n"
     ]
    }
   ],
   "source": [
    "print(spatial_size(50, 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 200, 100, 1)  \n",
    "        self.conv2 = nn.Conv2d(200, 50, 25, 1)  \n",
    "        self.fc1 = nn.Linear(13*13*50, 32)\n",
    "        self.fc2 = nn.Linear(32, 15)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x)) # torch.Size([5, 1, 200, 200]) ==> torch.Size([5, 200, 101, 101])\n",
    "        x = F.max_pool2d(x, 2, 2) # torch.Size([5, 200, 101, 101]) ==> torch.Size([5, 200, 50, 50])\n",
    "        x = F.relu(self.conv2(x)) # torch.Size([5, 200, 50, 50]) ==> torch.Size([5, 50, 26, 26])\n",
    "        x = F.max_pool2d(x, 2, 2) # torch.Size([5, 50, 26, 26]) ==> torch.Size([5, 50, 13, 13])\n",
    "        x = x.view(-1, 13*13*50)  # torch.Size([5, 50, 13, 13]) ==> torch.Size([5, 8450])\n",
    "        x = F.relu(self.fc1(x))   # torch.Size([5, 8450]) ==> torch.Size([5, 32])\n",
    "        x = self.fc2(x)           # torch.Size([5, 32]) ==> torch.Size([5, 15])\n",
    "        # # There's no activation at the final layer because of the criterion of CEL\n",
    "#         return x\n",
    "        return torch.log_softmax(x, dim=-1)\n",
    "\n",
    "\n",
    "net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.NLLLoss()\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,     1] loss: 16.42096\n",
      "[1,     2] loss: 7048195.42096\n",
      "[1,     3] loss: 9058073.42096\n",
      "[1,     4] loss: 9203302.17096\n",
      "[1,     5] loss: 9235659.11432\n",
      "[1,     6] loss: 9407265.22369\n",
      "[1,     7] loss: 9436942.47955\n",
      "[1,     8] loss: 9716284.32330\n",
      "[1,     9] loss: 9890782.10455\n",
      "[1,    10] loss: 9957298.92486\n",
      "[1,    11] loss: 9957301.53093\n",
      "[1,    12] loss: 9957304.08569\n",
      "[1,    13] loss: 9957306.63232\n",
      "[1,    14] loss: 9958839.13891\n",
      "[1,    15] loss: 9958841.75833\n",
      "[1,    16] loss: 9958844.30677\n",
      "[1,    17] loss: 9958846.79561\n",
      "[1,    18] loss: 9977143.56514\n",
      "[1,    19] loss: 9977146.07202\n",
      "[1,    20] loss: 9977148.62819\n",
      "[1,    21] loss: 9977151.12402\n",
      "[1,    22] loss: 9977153.65680\n",
      "[1,    23] loss: 9977156.20601\n",
      "[1,    24] loss: 9977158.62785\n",
      "[1,    25] loss: 9977161.03367\n",
      "[1,    26] loss: 9977163.52749\n",
      "[1,    27] loss: 9977165.95432\n",
      "[1,    28] loss: 9978689.93894\n",
      "[1,    29] loss: 9978692.39836\n",
      "[1,    30] loss: 9978694.77047\n",
      "[1,    31] loss: 9978697.06743\n",
      "[1,    32] loss: 9978699.61559\n",
      "[1,    33] loss: 9978702.07555\n",
      "[1,    34] loss: 9980470.47240\n",
      "[1,    35] loss: 9980472.82739\n",
      "[1,    36] loss: 9980475.19684\n",
      "[1,    37] loss: 9980477.66018\n",
      "[1,    38] loss: 9980480.06853\n",
      "[1,    39] loss: 9980482.22505\n",
      "[1,    40] loss: 9980484.39266\n",
      "[1,    41] loss: 9980486.57549\n",
      "[1,    42] loss: 9980488.90555\n",
      "[1,    43] loss: 9980491.41414\n",
      "[1,    44] loss: 9980493.77934\n",
      "[1,    45] loss: 9980495.95200\n",
      "[1,    46] loss: 9980498.09169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
      "    send_bytes(obj)\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
      "    self._send_bytes(m[offset:offset + size])\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
      "    self._send(header + buf)\n",
      "  File \"/Users/senhmo/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
      "    n = write(self._handle, buf)\n",
      "BrokenPipeError: [Errno 32] Broken pipe\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-7b957ff0b981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'image'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;31m#         print(outputs.shape, data['label'].shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-136-c1ab06bee6d6>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([5, 1, 200, 200]) ==> torch.Size([5, 200, 101, 101])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([5, 200, 101, 101]) ==> torch.Size([5, 200, 50, 50])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# torch.Size([5, 200, 50, 50]) ==> torch.Size([5, 50, 26, 26])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(1):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "#         inputs, labels = data['image'], data['label']\n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(data['image'])\n",
    "#         print(outputs.shape, data['label'].shape)\n",
    "        loss = criterion(outputs, data['label'])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        print('[%d, %5d] loss: %.5f' %\n",
    "              (epoch + 1, i + 1, running_loss / (epoch*i+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
